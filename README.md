# flash-attention
To scale trasformer-based algorithm efficiently
