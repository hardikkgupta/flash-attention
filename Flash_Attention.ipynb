{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Forward Pass**"
      ],
      "metadata": {
        "id": "uzgXDJ6AQsnf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4MG4Y9RIQlFj"
      },
      "outputs": [],
      "source": [
        "# dependencies\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_fwd_inner(\n",
        "    O_block,\n",
        "    l_i,\n",
        "    m_i,\n",
        "    Q_block,\n",
        "    K_block_ptr,\n",
        "    V_block_ptr,\n",
        "    block_index_q,\n",
        "    softmax_scale,\n",
        "    BLOCK_SIZE_Q: tl.constexpr,\n",
        "    BLOCK_SIZE_KV: tl.constexpr,\n",
        "    STAGE: tl.constexpr,\n",
        "    offs_q: tl.constexpr,\n",
        "    offs_kv: tl.constexpr,\n",
        "    SEQ_LEN: tl.constexpr,\n",
        "    ):\n",
        "  # range of values handled by this stage\n",
        "  if STAGE == 1:\n",
        "    # From 0 to the left of the diagonal (in the notebook)\n",
        "    lo, hi = 0, block_index_q * BLOCK_SIZE_Q\n",
        "  elif STAGE == 2:\n",
        "    # Used only for the block in which there is transition between non-masked and masked keys\n",
        "    lo, hi = block_index_q * BLOCK_SIZE_Q, (block_index_q + 1) * BLOCK_SIZE_Q\n",
        "    lo = tl.multiple_of(lo, BLOCK_SIZE_Q)\n",
        "  else:\n",
        "    # Only used for non-causal attention\n",
        "    lo, hi = 0, SEQ_LEN\n",
        "\n",
        "  # point it to first K, V block\n",
        "  K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n",
        "  V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n",
        "\n",
        "  # loop over k, v and update accumulator\n",
        "  for start_kv in range(lo, hi, BLOCK_SIZE_KV):\n",
        "    # Just let the compiler know that start_n is a multiple of BLOCK_N, so the compiler can do optimizations\n",
        "    start_kv = tl.multiple_of(start_kv, BLOCK_SIZE_KV)\n",
        "\n",
        "    # -- compute qk ----\n",
        "    K_block = tl.load(K_block_ptr)\n",
        "    QK_block = tl.dot(Q_block, K_block)\n",
        "\n",
        "    if STAGE == 2:\n",
        "      mask = offs_q[:, None] >= (start_kv + offs_kv[None, :])\n",
        "      QK_block = QK_block * softmax_scale + tl.where(mask, 0, -1.0e6)\n",
        "      m_ij = tl.maximum(m_i, tl.max(QK_block, 1))\n",
        "      QK_block -= m_ij[:, None]\n",
        "    else:\n",
        "      # Compute the maximum value of qk or keep the old max value\n",
        "      m_ij = tl.maximum(m_i, tl.max(QK_block, 1) * softmax_scale)\n",
        "      QK_block = QK_block * softmax_scale - m_ij[:, None]\n",
        "    # Compute the exponential of each dot product, so now we are computing exp(qk_ij - m_ij)\n",
        "    P_block = tl.math.exp(QK_block)\n",
        "\n",
        "    # Compute the sum by rows of the attention scores\n",
        "    l_ij = tl.sum(P_block, 1)\n",
        "\n",
        "    # This is the correction factor for the previous l_i\n",
        "    alpha = tl.math.exp(m_i - m_ij)\n",
        "\n",
        "    # Apply the correction factor to the previous l_i and add the new l_ij\n",
        "    l_i = l_i * alpha + l_ij\n",
        "\n",
        "    V_block = tl.load(V_block_ptr)\n",
        "    P_block = P_block.to(tl.float16)\n",
        "\n",
        "    # This computes the following: O_new = P x V + O_old * alpha\n",
        "    O_block = O_block * alpha[:, None]\n",
        "    O_block = tl.dot(P_block, V_block, O_block) # O_block += P_block @ V_block\n",
        "\n",
        "    m_i = m_ij\n",
        "\n",
        "    V_block_ptr = tl.advance(V_block_ptr, (BLOCK_SIZE_KV, 0)) # V[SEQ_LEN, HEAD_DIM]\n",
        "    K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_SIZE_KV)) # K[HEAD_DIM, SEQ_LEN]\n",
        "\n",
        "  return O_block, l_i, m_i"
      ],
      "metadata": {
        "id": "uXVj7u0Ebyj8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Triton kernel signature - makes the python method a triton kernel\n",
        "@triton.jit\n",
        "def _attn_fwd(\n",
        "    Q,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM # This is a pointer but we need to do something like Q[index_batch, index_head, :, :]\n",
        "    # done by qvk_offset\n",
        "    K,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
        "    V,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
        "    softmax_scale,\n",
        "    M,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN\n",
        "    O,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
        "    stride_Q_batch,\n",
        "    stride_Q_head,\n",
        "    stride_Q_seq,\n",
        "    stride_Q_dim,\n",
        "    stride_K_batch,\n",
        "    stride_K_head,\n",
        "    stride_K_seq,\n",
        "    stride_K_dim,\n",
        "    stride_V_batch,\n",
        "    stride_V_head,\n",
        "    stride_V_seq,\n",
        "    stride_V_dim,\n",
        "    stride_O_batch,\n",
        "    stride_O_head,\n",
        "    stride_O_seq,\n",
        "    stride_O_dim,\n",
        "    BATCH_SIZE,\n",
        "    NUM_HEADS: tl.constexpr,\n",
        "    SEQ_LEN: tl.constexpr,\n",
        "    HEAD_DIM: tl.constexpr,\n",
        "    BLOCK_SIZE_Q: tl.constexpr,\n",
        "    BLOCK_SIZE_KV: tl.constexpr,\n",
        "    STAGE: tl.constexpr,\n",
        "):\n",
        "  tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)\n",
        "\n",
        "  # This indicate which block in the sequence length to process\n",
        "  block_index_q = tl.program_id(0)\n",
        "\n",
        "  # This indicates which head and batch to process. Each program is associated with a single head of a single batch\n",
        "  index_batch_head = tl.program_id(1)\n",
        "  # This indicate which batch this program is associated with (each batch has NUM_HEADS heads)\n",
        "  index_batch = index_batch_head // NUM_HEADS\n",
        "  # This indicate the position of the head in the batch\n",
        "  index_head = index_batch_head % NUM_HEADS\n",
        "\n",
        "  qvk_offset = (\n",
        "      index_batch.to(tl.int64) * stride_Q_batch\n",
        "      + index_head.to(tl.int64) * stride_Q_head\n",
        "  )\n",
        "\n",
        "  # Make block pointer - takes a ptr # Q[index_batch, index_batch, block_index_q * BLOCK_SIZE_Q, :]\n",
        "  Q_block_ptr = tl.make_block_ptr(\n",
        "      base=Q + qvk_offset,\n",
        "      shape=(SEQ_LEN, HEAD_DIM),\n",
        "      strides=(stride_Q_seq, stride_Q_dim),\n",
        "      offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
        "      block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
        "      order=(1, 0),\n",
        "  )\n",
        "\n",
        "  # We are not skipping anything on sequence, HEAD_DIIM\n",
        "  V_block_ptr = tl.make_block_ptr( # V[index_batch, index_head, :, :]\n",
        "      base=V + qvk_offset,\n",
        "      shape=(SEQ_LEN, HEAD_DIM),\n",
        "      strides=(stride_V_seq, stride_V_dim),\n",
        "      offsets=(0, 0),\n",
        "      block_shape=(BLOCK_SIZE_KV, HEAD_DIM),\n",
        "      order=(1, 0),\n",
        "  )\n",
        "\n",
        "  K_block_ptr = tl.make_block_ptr( # K[index_batch, index_head, :, :]\n",
        "      base=K + qvk_offset,\n",
        "      shape=(HEAD_DIM, SEQ_LEN),\n",
        "      strides=(\n",
        "          stride_K_dim,\n",
        "          stride_K_seq,\n",
        "      ),  # We invert the strides w.r.t Q, so we transpose the matrix\n",
        "      offsets=(0, 0),\n",
        "      block_shape=(HEAD_DIM, BLOCK_SIZE_KV),\n",
        "      order=(0, 1),\n",
        "  )\n",
        "\n",
        "  O_block_ptr = tl.make_block_ptr( # O[index_batch, index_batch, block_index_q * BLOCK_SIZE_Q, :]\n",
        "      base=O + qvk_offset,\n",
        "      shape=(SEQ_LEN, HEAD_DIM),\n",
        "      strides=(stride_O_seq, stride_O_dim),\n",
        "      offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
        "      block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
        "      order=(1, 0),\n",
        "  )\n",
        "  # offs_q: the offsets for the tokens in the Q to process\n",
        "  offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)\n",
        "\n",
        "  # offs_kv: the offsets for the tokens in the K and V sequence to process\n",
        "  offs_kv = tl.arange(0, BLOCK_SIZE_KV)\n",
        "\n",
        "  # m_i: the running maximum. We have one for each query\n",
        "  m_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float(\"inf\")\n",
        "\n",
        "  # l_i: the running sum. We have one for each query (as we sum the attention scores by rows)\n",
        "  l_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0 # 1.0 to make it stable\n",
        "\n",
        "  # acc: the accumulator for the output, which is a group of rows of the O matrix\n",
        "  O_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)\n",
        "\n",
        "  # load it from HBM to shared memory\n",
        "  Q_block = tl.load(Q_block_ptr)\n",
        "\n",
        "\n",
        "  # Causal Attention: We dont want Q to attend K that come after it\n",
        "  if STAGE == 1 or STAGE == 3:\n",
        "    # This step runs for non-causal attention or for the blocks to the left of the diagonal in the causal attention\n",
        "    # _attn_fwd_inner this inner loop needs to go trhough all K, V blocks\n",
        "    # For each K, V it needs to fix previous calculated block\n",
        "    O_block, l_i, m_i = _attn_fwd_inner(\n",
        "          O_block,\n",
        "          l_i,\n",
        "          m_i,\n",
        "          Q_block,\n",
        "          K_block_ptr,\n",
        "          V_block_ptr,\n",
        "          block_index_q,\n",
        "          softmax_scale,\n",
        "          BLOCK_SIZE_Q,\n",
        "          BLOCK_SIZE_KV,\n",
        "          4 - STAGE,\n",
        "          offs_q,\n",
        "          offs_kv,\n",
        "          SEQ_LEN,\n",
        "      )\n",
        "  if STAGE == 3:\n",
        "    # This step runs for the blocks to the right of the diagonal in the causal attention\n",
        "    O_block, l_i, m_i = _attn_fwd_inner(\n",
        "          O_block,\n",
        "          l_i,\n",
        "          m_i,\n",
        "          Q_block,\n",
        "          K_block_ptr,\n",
        "          V_block_ptr,\n",
        "          block_index_q,\n",
        "          softmax_scale,\n",
        "          BLOCK_SIZE_Q,\n",
        "          BLOCK_SIZE_KV,\n",
        "          2,\n",
        "          offs_q,\n",
        "          offs_kv,\n",
        "          SEQ_LEN,\n",
        "      )\n",
        "\n",
        "  m_i += tl.math.log(\n",
        "      l_i\n",
        "  ) # This is needed to compute the logsumexp for the backwards pass\n",
        "  O_block = O_block / l_i[:, None]\n",
        "\n",
        "  # For each batch and each sequence length\n",
        "  m_ptrs = M + index_batch_head * SEQ_LEN + offs_q"
      ],
      "metadata": {
        "id": "1ON6dDHTpeTh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TritonAttention(torch.autograd.Function):\n",
        "\n",
        "  @staticmethod\n",
        "  # ctx = context storage for backward which is stored while forward\n",
        "  def forward(ctx, Q, K, V, causal, softmax_scale):\n",
        "    HEAD_DIM_Q, HEAD_DIM_K = Q.shape[-1], K.shape[-1]\n",
        "    HEAD_DIM_V = V.shape[-1]\n",
        "\n",
        "    BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM = Q.shape\n",
        "    assert HEAD_DIM == HEAD_DIM_Q and HEAD_DIM_K == HEAD_DIM_V\n",
        "\n",
        "    O = torch.empty_like(Q)\n",
        "    stage = 3 if causal else 1\n",
        "\n",
        "    # launch grid (tile) - how many parallel process to be launched\n",
        "    grid = lambda args: (\n",
        "        # ceil(SEQ_LEN / BLOCK_SIZE_Q)\n",
        "        triton.cdiv(SEQ_LEN, args[\"BLOCK_SIZE\"]), # which group of queries we are going to work with (how many tiles along the sequence)\n",
        "        BATCH_SIZE * NUM_HEADS, # which head of which batch element we are going to work with? (one “row” per (batch, head) pair)\n",
        "        1,\n",
        "    )\n",
        "\n",
        "    # M is the logsumexp for backward pass, one for each query\n",
        "    M = torch.empty(\n",
        "        (BATCH_SIZE, NUM_HEADS, SEQ_LEN), device=Q.device, dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    # Triton Kernel\n",
        "    _attn_fwd[grid](\n",
        "            Q=Q, # just starting pointer\n",
        "            K=K,\n",
        "            V=V,\n",
        "            softmax_scale=softmax_scale,\n",
        "            M=M,\n",
        "            O=O,\n",
        "            stride_Q_batch=Q.stride(0),\n",
        "            stride_Q_head=Q.stride(1),\n",
        "            stride_Q_seq=Q.stride(2),\n",
        "            stride_Q_dim=Q.stride(3),\n",
        "            stride_K_batch=K.stride(0),\n",
        "            stride_K_head=K.stride(1),\n",
        "            stride_K_seq=K.stride(2),\n",
        "            stride_K_dim=K.stride(3),\n",
        "            stride_V_batch=V.stride(0),\n",
        "            stride_V_head=V.stride(1),\n",
        "            stride_V_seq=V.stride(2),\n",
        "            stride_V_dim=V.stride(3),\n",
        "            stride_O_batch=O.stride(0),\n",
        "            stride_O_head=O.stride(1),\n",
        "            stride_O_seq=O.stride(2),\n",
        "            stride_O_dim=O.stride(3),\n",
        "            BATCH_SIZE=Q.shape[0],\n",
        "            NUM_HEADS=Q.shape[1],\n",
        "            SEQ_LEN=Q.shape[2],\n",
        "            HEAD_DIM=HEAD_DIM_K,\n",
        "            STAGE=stage,\n",
        "        )\n",
        "\n",
        "    ctx.save_for_backward(Q, K, V, O, M)\n",
        "    ctx.grid = grid\n",
        "    ctx.softmax_scale = softmax_scale\n",
        "    ctx.HEAD_DIM = HEAD_DIM_K\n",
        "    ctx.causal = causal\n",
        "    return O\n"
      ],
      "metadata": {
        "id": "08EuPiCufxsf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise query, key and value sequence\n",
        "def test_op(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, causal, dtype=torch.float16):\n",
        "  Q = (\n",
        "      torch.empty(\n",
        "          (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM),\n",
        "          dtype=dtype, device=\"cuda\"\n",
        "      )\n",
        "      .normal_(mean=0.0, std=0.5)\n",
        "      .requires_grad_()\n",
        "  )\n",
        "  K = (\n",
        "      torch.empty(\n",
        "          (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM),\n",
        "          dtype=dtype, device=\"cuda\"\n",
        "      )\n",
        "      .normal_(mean=0.0, std=0.5)\n",
        "      .requires_grad_()\n",
        "  )\n",
        "  V = (\n",
        "      torch.empty(\n",
        "          (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM),\n",
        "          dtype=dtype, device=\"cuda\"\n",
        "      )\n",
        "      .normal_(mean=0.0, std=0.5)\n",
        "      .requires_grad_()\n",
        "  )"
      ],
      "metadata": {
        "id": "-eXw4dCPV6yx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling factor\n",
        "# QK^t/sqrt(HEAD_DIM)\n",
        "softmax_scale = 1 / (HEAD_DIM ** 0.5)\n",
        "d0 = torch.randn_like(Q) # for backward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "9VAG6tqEXMKC",
        "outputId": "8aab0f88-b63b-4192-90fb-b8b8fc19c834"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'HEAD_DIM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-382806663.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# scaling factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# QK^t/sqrt(HEAD_DIM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msoftmax_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mHEAD_DIM\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0md0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'HEAD_DIM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Attention\n",
        "MASK = torch.tril(torch.ones((SEQ_LEN, SEQ_LEN), device = \"cuda\"))\n",
        "P = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale # swaps dim 2 with dim 3\n",
        "if causal:\n",
        "  P[:, :, MASK == 0] = float(\"-inf\")\n",
        "P = torch.softmax(P.float(), dim = -1).half()\n",
        "\n",
        "ref_0 = torch.matmul(P, V)\n",
        "ref_0.backward(d0)\n",
        "ref_dV, V.grad = V.grad.clone(), None\n",
        "ref_dK, K.grad = K.grad.clone(), None\n",
        "ref_dQ, Q.grad = Q.grad.clone(), None\n",
        "\n",
        "# Triton Implementation\n",
        "tri_out = TritonAttention.apply(Q, K, V, causal, softmax_scale).half()\n",
        "tri_out.backward(d0)\n",
        "tri_dV, V.grad = V.grad.clone(), None\n",
        "tri_dK, K.grad = K.grad.clone(), None\n",
        "tri_dQ, Q.grad = Q.grad.clone(), None\n",
        "\n",
        "# compare\n",
        "rtol = 0.0 # relative tolerance\n",
        "atol = 1e-2 # absolute tolerance\n",
        "assert torch.allclose(ref_0, tri_out, rtol=rtol, atol=atol)\n",
        "assert torch.allclose(ref_dV, tri_dV, rtol=rtol, atol=atol)\n",
        "assert torch.allclose(ref_dK, tri_dK, rtol=rtol, atol=atol)\n",
        "assert torch.allclose(ref_dQ, tri_dQ, rtol=rtol, atol=atol)"
      ],
      "metadata": {
        "id": "PuKr0RmJb9QX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}